{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections \n",
    "import os\n",
    "import tarfile \n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "sess = tf.Session()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We declare some model parameters. We will look at 50 pairs of word embeddings at a time (batch size). The embedding size\n",
    "#of each word will be a vector of length 200, and we will only consider the 10,000 most frequent words (every other word\n",
    "#will be classified as unknown). We will train for 50,000 generations and print out the loss function, and we also declare\n",
    "#our skip-gram window size. Here we set our window size to two, so we will look at the surrounding two words on each side\n",
    "#of the target. We set our stopwords from the Python package nltk. We also want a way to check how our word embeddings\n",
    "#are performing, so we choose some common movie review words and we will print out the nearest neighbor words from these\n",
    "#every 2,000 iterations \n",
    "batch_size = 50\n",
    "embedding_size = 200\n",
    "vocabulary_size = 10000\n",
    "generations = 50000\n",
    "print_loss_every = 500\n",
    "num_sampled = int(batch_size/2)\n",
    "window_size = 2\n",
    "stops = stopwords.words('english')\n",
    "print_valid_every = 2000\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we declare our data loading function, which checks to make sure we have not downloaded the data before it downloads,\n",
    "#or it will load the data from the disk if we have saved it before\n",
    "def load_movie_data():\n",
    "    save_folder_name = 'rt-polaritydata'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
    "\n",
    "    # Check if files are already downloaded\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "\n",
    "        # Save tar.gz file\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        # Extract tar.gz file into temp folder\n",
    "        tar = tarfile.open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path='temp')\n",
    "        tar.close()\n",
    "\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    pos_data = [x.rstrip() for x in pos_data]\n",
    "\n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    neg_data = [x.rstrip() for x in neg_data]\n",
    "    \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    \n",
    "    return(texts, target)\n",
    "\n",
    "\n",
    "texts, target = load_movie_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we crate a normalization function for text. This function will input a list of strings and apply lowecasing, remove\n",
    "#punctiation, remove numbers, trim extra whitespace, and remove stop words. \n",
    "def normalize_text(texts, stops):\n",
    "    #Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "    #Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    #Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    #Remove stopwords\n",
    "    texts = [' '.join(word for word in x.split() if word not in (stops)) for x in texts]\n",
    "    #Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)\n",
    "texts = normalize_text(texts, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make sure that all our movie reviews are informative, we should make sure they are long enough to contain important\n",
    "#word relationships. We arbitrarily set this to three or more words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To build our vocabulary, we create a function that creates a dictionary of words with their count, and any word that is\n",
    "#uncommon enough to not make our vocabulary size cut-off, we label as 'RARE'\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    #Turn sentences (list of strings) into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    #Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [['RARE', -1]]\n",
    "    #Now add most frequent words, limited to the N-most frequent(N=vocabulary size)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    #Now create the dictionary\n",
    "    word_dict = {}\n",
    "    #For each word, that we want in the dictionary, add it, then make it the value of the prior dictionary length\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need a function that will convert a list of sentences into lists of word indices that we can pass into our embedding \n",
    "#lookup function\n",
    "def text_to_numbers(sentences, word_dict):\n",
    "    #Initialize the returned data \n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        #For each word, either use selected index or rare word index\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can actually create our dictionary and transform our list of sentences into list of word indices\n",
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the preceding word dictionary, we can look up the index for the validation words we choose before\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now create a function that will return our skip-gram batches. We want to train on pairs of words where one word\n",
    "#is the training input (from the target word at the center of our window) and the other word is selected from the \n",
    "#window. For example, the sentence \"the cat in the hat\" may result in (input,output) pairs such as the following:\n",
    "#(the,in), (cat,in), (the,in), (hat,in), if in was the target word, and we had a window size of two in each direction\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return(batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can initialize our embedding matrix, and declare our placeholders and our embedding lookup function.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "#Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "#Lookup the word embedding\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The loss function should be something such as a softmax, which calculates the loss on predicting the wrong word category.\n",
    "#But since our target has 10,000 different categories, it is very sparse. This sparsity causes problems fitting or \n",
    "#converging for a model. To tackle this, we use a loss function called noise-contrastive error(NCE). This NCE loss\n",
    "#function turns our problem into a binary prediction, by predicting the word class versus random noise predictions. \n",
    "#The num_sampled parameter is how much of the batch to turn into random noise \n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to create a way to find nearby words to our validation words. We will do this by computing the cosine \n",
    "#similarity between the validatoin set and all of our word embeddings, then we can print out the closest set of words for\n",
    "#each validation word.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings,transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now declare our optimizer function, and initialize our model variables. \n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 28.444011688232422\n",
      "Loss at step 1000 : 19.690587997436523\n",
      "Loss at step 1500 : 28.480619430541992\n",
      "Loss at step 2000 : 1.0775953531265259\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 2500 : 4.748654842376709\n",
      "Loss at step 3000 : 2.215075969696045\n",
      "Loss at step 3500 : 4.729773044586182\n",
      "Loss at step 4000 : 2.888399600982666\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 4500 : 2.9385247230529785\n",
      "Loss at step 5000 : 18.377548217773438\n",
      "Loss at step 5500 : 0.5258269906044006\n",
      "Loss at step 6000 : 1.3805384635925293\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 6500 : 1.257286548614502\n",
      "Loss at step 7000 : 1.2049566507339478\n",
      "Loss at step 7500 : 7.100846767425537\n",
      "Loss at step 8000 : 2.9963865280151367\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 8500 : 1.4643439054489136\n",
      "Loss at step 9000 : 0.5735929608345032\n",
      "Loss at step 9500 : 3.097538709640503\n",
      "Loss at step 10000 : 0.9759653210639954\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 10500 : 1.6614160537719727\n",
      "Loss at step 11000 : 3.6532299518585205\n",
      "Loss at step 11500 : 3.5983498096466064\n",
      "Loss at step 12000 : 2.1183881759643555\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 12500 : 1.9554131031036377\n",
      "Loss at step 13000 : 1.9583919048309326\n",
      "Loss at step 13500 : 2.221797466278076\n",
      "Loss at step 14000 : 1.9014129638671875\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 14500 : 1.8651740550994873\n",
      "Loss at step 15000 : 0.14783960580825806\n",
      "Loss at step 15500 : 2.1795644760131836\n",
      "Loss at step 16000 : 2.3571786880493164\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 16500 : 0.16686847805976868\n",
      "Loss at step 17000 : 2.9673116207122803\n",
      "Loss at step 17500 : 2.895608425140381\n",
      "Loss at step 18000 : 1.2717671394348145\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 18500 : 1.403846025466919\n",
      "Loss at step 19000 : 2.0424671173095703\n",
      "Loss at step 19500 : 5.184993267059326\n",
      "Loss at step 20000 : 4.179515838623047\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 20500 : 3.208716630935669\n",
      "Loss at step 21000 : 1.593103289604187\n",
      "Loss at step 21500 : 0.9792978167533875\n",
      "Loss at step 22000 : 6.441169261932373\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 22500 : 2.1491174697875977\n",
      "Loss at step 23000 : 2.4820139408111572\n",
      "Loss at step 23500 : 1.0351070165634155\n",
      "Loss at step 24000 : 2.365963935852051\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 24500 : 0.8947246074676514\n",
      "Loss at step 25000 : 1.0498149394989014\n",
      "Loss at step 25500 : 1.94584059715271\n",
      "Loss at step 26000 : 1.898038387298584\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 26500 : 1.097761869430542\n",
      "Loss at step 27000 : 0.9040500521659851\n",
      "Loss at step 27500 : 2.3250081539154053\n",
      "Loss at step 28000 : 2.6148247718811035\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 28500 : 1.5971614122390747\n",
      "Loss at step 29000 : 1.3533351421356201\n",
      "Loss at step 29500 : 1.0419377088546753\n",
      "Loss at step 30000 : 0.9169046878814697\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 30500 : 1.8637144565582275\n",
      "Loss at step 31000 : 1.0815626382827759\n",
      "Loss at step 31500 : 1.512155294418335\n",
      "Loss at step 32000 : 1.9505823850631714\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 32500 : 1.7681927680969238\n",
      "Loss at step 33000 : 2.0003206729888916\n",
      "Loss at step 33500 : 1.1483933925628662\n",
      "Loss at step 34000 : 2.4754326343536377\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 34500 : 2.330012559890747\n",
      "Loss at step 35000 : 2.638479709625244\n",
      "Loss at step 35500 : 8.656667709350586\n",
      "Loss at step 36000 : 2.213909149169922\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 36500 : 0.8065575957298279\n",
      "Loss at step 37000 : 2.987954616546631\n",
      "Loss at step 37500 : 1.5680650472640991\n",
      "Loss at step 38000 : 2.871208906173706\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 38500 : 1.2031164169311523\n",
      "Loss at step 39000 : 4.00250244140625\n",
      "Loss at step 39500 : 1.1819109916687012\n",
      "Loss at step 40000 : 1.5188648700714111\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 40500 : 1.5759927034378052\n",
      "Loss at step 41000 : 0.5761818885803223\n",
      "Loss at step 41500 : 1.6501269340515137\n",
      "Loss at step 42000 : 1.0966615676879883\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 42500 : 1.9843988418579102\n",
      "Loss at step 43000 : 2.0227510929107666\n",
      "Loss at step 43500 : 0.29960528016090393\n",
      "Loss at step 44000 : 1.5518510341644287\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 44500 : 1.3485057353973389\n",
      "Loss at step 45000 : 1.433596134185791\n",
      "Loss at step 45500 : 1.5545991659164429\n",
      "Loss at step 46000 : 1.202038288116455\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 46500 : 1.2333775758743286\n",
      "Loss at step 47000 : 1.206703543663025\n",
      "Loss at step 47500 : 1.0608783960342407\n",
      "Loss at step 48000 : 1.007881760597229\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n",
      "Loss at step 48500 : 0.9326512217521667\n",
      "Loss at step 49000 : 3.222097158432007\n",
      "Loss at step 49500 : 2.0290732383728027\n",
      "Loss at step 50000 : 2.013237714767456\n",
      "Nearest to cliche: hermetic, huppert, satirical, assuredly, relocation,\n",
      "Nearest to love: assassination, cuban, hanks, friend, rapturous,\n",
      "Nearest to hate: male, art, dense, unbearable, mismo,\n",
      "Nearest to silly: pokes, shots, nuances, accent, theatres,\n",
      "Nearest to sad: died, guitar, hawn, brilliant, kane,\n"
     ]
    }
   ],
   "source": [
    "#Now we can print our embeddings and print off the loss and closest words to our validations set during the training.\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                score = sim[j,nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

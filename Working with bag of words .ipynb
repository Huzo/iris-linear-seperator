{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn \n",
    "sess = tf.Session()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_name = os.path.join('smsspamcollection','SMSSpamCollection.csv')\n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name, 'r') as temp_output_file:\n",
    "        reader = csv.reader(temp_output_file)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    #Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    \n",
    "    #And write to csv\n",
    "    with open(save_file_name, 'w') as temp_output_file:\n",
    "        writer = csv.writer(temp_output_file)\n",
    "        writer.writerows(text_data)\n",
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]\n",
    "#Relabel 'spam' as 1 'ham' as 0\n",
    "target = [1 if x=='spam' else 0 for x in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reduce the potential vocabulary size, we normalize the text. To do this, we remove the influence of capitalization\n",
    "#and numbers in the text. \n",
    "\n",
    "#Convert to lower case\n",
    "texts = [x.lower() for x in texts]\n",
    "#Remove punctuation\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "#Remove numbers\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "#Trim extra whitespace \n",
    "texts = [' '.join(x.split()) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAGltJREFUeJzt3X+UXGWd5/H3hwTklxIIbQaTaOMQQcYdEHsxDq4HE53lh5DMrnJgVYKT2exZcUYHXc14ZkRn1jlh1hFhnWFORpTgMEBE2OQI6042hEF2AO0A8puhxcSkzY82kmDEX5Hv/nG/DZemO13VXZVKP/15nVOnnvvc59Z9nkr1p26eulVXEYGZmZXrgE53wMzM2stBb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAf9BCXpEUmnd7ofnSTp9yRtkrRb0hv30T6Pl/SApJ9I+qN9sc9R+nORpLvGsN3/lrSoHX2y/Y+Dfj8kaYOkdwype9EfdET8VkTcMcrjdEsKSVPb1NVO+xzwoYg4PCLu31tDSf2SDpE0T9LN49jnx4F1EfHyiLhyyD4ukPTYkLo1I9QtHUcfxi0izoyIFc1ul2+qg7fnJP2stvzesfZH0sH5Wp011sewkTnobcz2gzeQ1wCPjNZI0mxgR0T8DHgTcF+b9nkncIKkrtzvVOAk4JAhdW/Jtk2RNGVMPW6hfFM9PCIOB34AnFOru67T/bPhOegnqPpRv6RTJfVKekbSNkmfz2aDYbIzj7jeIukASX8qaaOk7ZKulXRE7XEvzHU7JP3ZkP18WtJNkv5B0jPARbnvuyXtlLRF0hclHVR7vJD0QUlP5nTHX0j6TUn/kv1dWW8/ZIzD9lXSyyTtBqYA35X0vVGerh5gfa2816CXdG5Oje2UdIek12f97cDbgS/m8/m6+nYR0Q88Bbwtq06helP45yF1BwDfycd8fe5jZ+7z3Fo/rpF0laTbJP0UeLuk6ZJW53P3beA3a+0l6fJ8rp6R9JCkN4wwxjsk/UGWL5J0l6TPSXpa0vclnbn3p3TE525Kvm6ekvQjSddJmpbrFkn6V0mH5fLvSdos6UheeK0+kc/tQkm/Iemb+dzsyOffxiIifNvPbsAG4B1D6i4C7hquDXA38P4sHw7MzXI3EMDU2na/D/QBr822NwNfzXUnAruBtwIHUU2N/Kq2n0/n8kKqsDqE6gh5LjA19/cY8JHa/gJYBbwC+C3gF8Da3P8RwKPAohGehxH7Wnvs4/byPF4K7AR+Djyb5V8Du7I8ZZhtXgf8FHgncCDVVE0fcFCuvwP4g73s8yvAFVn+GPDnwH8eUnd7lg/Mx/5kPt/zgJ8Ax+f6a7Kvp+XzfTBwA7ASOAx4A9A/+LoA/j3VG9o0QMDrgWNG6Ofz46B6bf0q+zkF+K/ADwGN4XX6CeBbwKuyv9cAX6mt/zrwd8AMYBvwzqw/OP89Z9XaXg5cka+tg4C3dfpvc6LeOt4B34b5R6n+gHZnGA3enmXkoL8T+Axw9JDH6ealQb8W+GBt+fj8I58KfAq4vrbuUOCXvDjo7xyl7x8BbqktB3BabXk98Ina8l8DXxjhsUbsa+2xRwz6bDOV6s1nBvA7wK2jtP8zYGVt+YAM09Nz+fmAHGH7i4D7s7yK6g3jhCF1l2b53wFbgQNq218PfDrL1wDX1tZNyfGfUKv7S14I+nnAv1K98R4wyjifH0f2uW/Iv3sAv9HA63Ro0H9/yL/3sfnaVS5Pp3oTeZh888v64YL+r4CvAa/t9N/kRL956mb/tTAipg3egA/upe1iqiPRxyV9R9K79tL2VcDG2vJGqjCckes2Da6IiGeBHUO231RfkPQ6Sd+QtDWnc/4SOHrINttq5Z8Ns3z4GPq6V5JOlrQTeBo4DngCWAecnlMB/6GRfUbEc1RjnjnaPtOdwG/ndMRc4O6IeBw4JuveygvTFK8CNuU+6mOs76v+fHdRjX/TkPaDfb0d+CLwN8B2ScslvaLBfm+tPc6zWRzp32VYkgTMBm7L53gncD/Vm+X0fOwdwC1U/3v8/EiPlT5L9aawTlKfpEua6Y+9wEFfgIh4MiIuAF4JXAbclPOgw/006Q+pPlAc9GpgD1X4bgGeP+tB0iHkH2h9d0OWrwIeB+ZExCuopiE09tE03Ne9iogH8g3ys8CnsvwocFK+eY505s2L9lkLr/5GOhwRT+VjLAF+EBG7c9XdWXc4cE9tX7Ml1f8OXz1kX/Xne4Bq/LOHtK/v/8qIeBNVkL4O+G+N9LsVojoM7wfm1Q9SIuLgiPgRVJ8nARdQHanXz1p6yWs1InZFxIcj4jXAfwT+VNJp7R9JeRz0BZD0PkldeWS4M6ufowqG56jmuAddD/yxpGMlHU51BH5jROwBbgLOkfQ7+QHppxk9tF8OPAPslnQC1fxuq+ytr416E3BfjudVEdE3SvuVwNmS5ks6EPgo1ecK/9LEPr8FXJL3g+7Kut6ozv4BuJdqWuPjkg5U9b2Ic6jm4V8iIn5N9TnFpyUdKulE4Plz4SX9W0lvzn7/lOqzieeGe6w2+jtgmaoznZD0SknnZPlQ4B+ontOLgOMl/T5ARPyC6vOI51+r+aH4a/PNdhfV5yv7ejxFcNCX4QzgkTwT5Qrg/Ij4Wf4X/LPA/8v/Ss8Fvgx8lWr64PtUYfCHABHxSJZvoDq63w1spwq6kXwM+E9UHyL+PXBjC8c1Yl+bMHg65b+hmhfeq4h4Angf8D+BH1EF7zkR8csm9vnPVP+7qn+R6VtZ9/xplfmY5wBn5r7+Frgwp3pG8iGq/xVsJT/orK17BdW/wdNUUzo7gP/RRL9b4a+A/wvcLuknVG+Qp+S6vwYejYiv5Jvd+4HPSerO9Z8Cvpav1XOpPkxeR/XauhP4XETcvc9GUpDBD0jMXiKPondSTct8v9P9MbOx8RG9vYikc3Ja4DCq0ysfojq7wswmKAe9DbWA6kPCHwJzqKaB/N8+swnMUzdmZoXzEb2ZWeE6/aNUABx99NHR3d3d6W6YmU0o69ev/1FEdI3Wbr8I+u7ubnp7ezvdDTOzCUXSxtFbeerGzKx4Dnozs8I56M3MCuegNzMrnIPezKxwDnozs8I56M3MCuegNzMrnIPezKxw+8U3Y0vTvfTWptpvWHZ2m3piZuYjejOz4jUU9JL+WNIjkh6WdL2kg/M6nvfm1dlvzGtyIulludyX67vbOQAzM9u7UYNe0kzgj4CeiHgDMAU4H7gMuDwijqO6RuXi3GQx8HTWX57tzMysQxqdupkKHCJpKnAo1YWj5wE35foVwMIsL8hlcv38vIq7mZl1wKhBHxH9VNcO/QFVwO8C1gM7I2JPNtsMzMzyTGBTbrsn208f+riSlkjqldQ7MDAw3nGYmdkIGpm6OZLqKP1Y4FXAYcAZ491xRCyPiJ6I6OnqGvV3883MbIwambp5B/D9iBiIiF8BNwOnAdNyKgdgFtCf5X5gNkCuPwLY0dJem5lZwxoJ+h8AcyUdmnPt84FHgXXAu7PNImBVllfnMrn+9vAVyM3MOqaROfp7qT5UvQ94KLdZDnwCuERSH9Uc/NW5ydXA9Ky/BFjahn6bmVmDGvpmbERcClw6pPop4NRh2v4ceM/4u2ZmZq3gb8aamRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRXOQW9mVjgHvZlZ4Rz0ZmaFc9CbmRXOQW9mVjgHvZlZ4Rz0ZmaFa+Ti4MdLeqB2e0bSRyQdJWmNpCfz/shsL0lXSuqT9KCkU9o/DDMzG0kjlxJ8IiJOjoiTgTcBzwK3UF0icG1EzAHW8sIlA88E5uRtCXBVOzpuZmaNaXbqZj7wvYjYCCwAVmT9CmBhlhcA10blHmCapGNa0lszM2tas0F/PnB9lmdExJYsbwVmZHkmsKm2zeasexFJSyT1SuodGBhoshtmZtaohoNe0kHAucDXhq6LiACimR1HxPKI6ImInq6urmY2NTOzJjRzRH8mcF9EbMvlbYNTMnm/Pev7gdm17WZlnZmZdUAzQX8BL0zbAKwGFmV5EbCqVn9hnn0zF9hVm+IxM7N9bGojjSQdBrwT+C+16mXASkmLgY3AeVl/G3AW0Ed1hs4HWtZbMzNrWkNBHxE/BaYPqdtBdRbO0LYBXNyS3pmZ2bj5m7FmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVr6Jux1l7dS29tqv2GZWe3qSdmViIf0ZuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWuIaCXtI0STdJelzSY5LeIukoSWskPZn3R2ZbSbpSUp+kByWd0t4hmJnZ3jR6RH8F8M2IOAE4CXgMWAqsjYg5wNpchuoi4nPytgS4qqU9NjOzpowa9JKOAN4GXA0QEb+MiJ3AAmBFNlsBLMzyAuDaqNwDTJN0TMt7bmZmDWnkiP5YYAD4iqT7JX0pLxY+IyK2ZJutwIwszwQ21bbfnHUvImmJpF5JvQMDA2MfgZmZ7VUjQT8VOAW4KiLeCPyUF6ZpgOcvCB7N7DgilkdET0T0dHV1NbOpmZk1oZGg3wxsjoh7c/kmquDfNjglk/fbc30/MLu2/aysMzOzDhj1R80iYqukTZKOj4gngPnAo3lbBCzL+1W5yWrgQ5JuAN4M7KpN8UxIzf7omJnZ/qTRX6/8Q+A6SQcBTwEfoPrfwEpJi4GNwHnZ9jbgLKAPeDbbmplZhzQU9BHxANAzzKr5w7QN4OJx9svMzFrE34w1Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3Myucg97MrHAOejOzwjnozcwK56A3MytcQ0EvaYOkhyQ9IKk3646StEbSk3l/ZNZL0pWS+iQ9KOmUdg7AzMz2rpkj+rdHxMkRMXhJwaXA2oiYA6zNZYAzgTl5WwJc1arOmplZ88YzdbMAWJHlFcDCWv21UbkHmCbpmHHsx8zMxqHRoA/gnyStl7Qk62ZExJYsbwVmZHkmsKm27easexFJSyT1SuodGBgYQ9fNzKwRUxts99aI6Jf0SmCNpMfrKyMiJEUzO46I5cBygJ6enqa2NTOzxjV0RB8R/Xm/HbgFOBXYNjglk/fbs3k/MLu2+aysMzOzDhg16CUdJunlg2Xgd4GHgdXAomy2CFiV5dXAhXn2zVxgV22Kx8zM9rFGpm5mALdIGmz/jxHxTUnfAVZKWgxsBM7L9rcBZwF9wLPAB1reazMza9ioQR8RTwEnDVO/A5g/TH0AF7ekdzas7qW3NtV+w7Kz29QTM5sI/M1YM7PCOejNzArnoDczK5yD3syscA56M7PCOejNzArnoDczK5yD3syscA56M7PCOejNzArnoDczK5yD3syscA56M7PCOejNzArnoDczK5yD3syscA0HvaQpku6X9I1cPlbSvZL6JN0o6aCsf1ku9+X67vZ03czMGtHMEf2Hgcdqy5cBl0fEccDTwOKsXww8nfWXZzszM+uQhoJe0izgbOBLuSxgHnBTNlkBLMzyglwm18/P9mZm1gGNHtF/Afg48FwuTwd2RsSeXN4MzMzyTGATQK7fle1fRNISSb2SegcGBsbYfTMzG82oQS/pXcD2iFjfyh1HxPKI6ImInq6urlY+tJmZ1UxtoM1pwLmSzgIOBl4BXAFMkzQ1j9pnAf3Zvh+YDWyWNBU4AtjR8p6bmVlDRj2ij4g/iYhZEdENnA/cHhHvBdYB785mi4BVWV6dy+T62yMiWtprMzNr2HjOo/8EcImkPqo5+Kuz/mpgetZfAiwdXxfNzGw8Gpm6eV5E3AHckeWngFOHafNz4D0t6JuZmbWAvxlrZla4po7obWLqXnpr09tsWHZ2G3piZp3gI3ozs8I56M3MCuegNzMrnIPezKxwDnozs8I56M3MCuegNzMrnIPezKxwDnozs8I56M3MCuegNzMrnH/rxobV7O/j+LdxzPZfPqI3Myucg97MrHCNXBz8YEnflvRdSY9I+kzWHyvpXkl9km6UdFDWvyyX+3J9d3uHYGZme9PIEf0vgHkRcRJwMnCGpLnAZcDlEXEc8DSwONsvBp7O+suznZmZdUgjFwePiNidiwfmLYB5wE1ZvwJYmOUFuUyuny9JLeuxmZk1paE5eklTJD0AbAfWAN8DdkbEnmyyGZiZ5ZnAJoBcv4vq4uFDH3OJpF5JvQMDA+MbhZmZjaihoI+IX0fEycAsqguCnzDeHUfE8ojoiYierq6u8T6cmZmNoKmzbiJiJ7AOeAswTdLgefizgP4s9wOzAXL9EcCOlvTWzMyaNuoXpiR1Ab+KiJ2SDgHeSfUB6zrg3cANwCJgVW6yOpfvzvW3R0S0oe+2H9nfvmC1v/XHrJMa+WbsMcAKSVOo/gewMiK+IelR4AZJ/x24H7g6218NfFVSH/Bj4Pw29NvMzBo0atBHxIPAG4epf4pqvn5o/c+B97Skd2ap2SN0M3uBvxlrZlY4B72ZWeH865XWEZ6KMdt3fERvZlY4B72ZWeE8dWPG2KaSfO69TRQOerMx8peybKLw1I2ZWeEc9GZmhXPQm5kVzkFvZlY4B72ZWeEc9GZmhXPQm5kVzkFvZlY4B72ZWeFGDXpJsyWtk/SopEckfTjrj5K0RtKTeX9k1kvSlZL6JD0o6ZR2D8LMzEbWyBH9HuCjEXEiMBe4WNKJwFJgbUTMAdbmMsCZwJy8LQGuanmvzcysYaMGfURsiYj7svwT4DFgJrAAWJHNVgALs7wAuDYq9wDTJB3T8p6bmVlDmpqjl9RNdf3Ye4EZEbElV20FZmR5JrCpttnmrBv6WEsk9UrqHRgYaLLbZmbWqIaDXtLhwNeBj0TEM/V1ERFANLPjiFgeET0R0dPV1dXMpmZm1oSGgl7SgVQhf11E3JzV2wanZPJ+e9b3A7Nrm8/KOjMz64BGzroRcDXwWER8vrZqNbAoy4uAVbX6C/Psm7nArtoUj5mZ7WONXHjkNOD9wEOSHsi6TwLLgJWSFgMbgfNy3W3AWUAf8CzwgZb22MzMmjJq0EfEXYBGWD1/mPYBXDzOfpmZWYv4m7FmZoVz0JuZFc5Bb2ZWOAe9mVnhHPRmZoVz0JuZFc5Bb2ZWuEa+MGVmLdC99Nam2m9YdnabemKTjY/ozcwK56A3Myucg97MrHAOejOzwk26D2Ob/UDMzGyi8xG9mVnhJt0RvdlE4dMxrVV8RG9mVrhGLiX4ZUnbJT1cqztK0hpJT+b9kVkvSVdK6pP0oKRT2tl5MzMbXSNH9NcAZwypWwqsjYg5wNpcBjgTmJO3JcBVremmmZmN1ahBHxF3Aj8eUr0AWJHlFcDCWv21UbkHmCbpmFZ11szMmjfWOfoZEbEly1uBGVmeCWyqtducdWZm1iHj/jA2LwYezW4naYmkXkm9AwMD4+2GmZmNYKxBv21wSibvt2d9PzC71m5W1r1ERCyPiJ6I6Onq6hpjN8zMbDRjDfrVwKIsLwJW1eovzLNv5gK7alM8ZmbWAaN+YUrS9cDpwNGSNgOXAsuAlZIWAxuB87L5bcBZQB/wLPCBNvTZzMyaMGrQR8QFI6yaP0zbAC4eb6fMzKx1/BMIZoXwTybYSPwTCGZmhXPQm5kVzkFvZlY4B72ZWeEc9GZmhXPQm5kVzkFvZla4CX8evS/2bWa2dxM+6M1sbPwFq8nDUzdmZoVz0JuZFc5TN2bWkLF8Hubpnv2Dj+jNzArnoDczK5yD3syscJ6jN7O28Smc+4e2BL2kM4ArgCnAlyJiWTv2Y2ZlafcXICfrG0nLg17SFOBvgHcCm4HvSFodEY+2el9mZu20L755vy/efNpxRH8q0BcRTwFIugFYADjozayjJutPprQj6GcCm2rLm4E3D20kaQmwJBd3S3pijPs7GvjRGLedyCbruGHyjt3jLpAuG3FVI+N+TSP76NiHsRGxHFg+3seR1BsRPS3o0oQyWccNk3fsHvfk0spxt+P0yn5gdm15VtaZmVkHtCPovwPMkXSspIOA84HVbdiPmZk1oOVTNxGxR9KHgP9DdXrllyPikVbvp2bc0z8T1GQdN0zesXvck0vLxq2IaNVjmZnZfsg/gWBmVjgHvZlZ4SZ00Es6Q9ITkvokLe10f9pF0pclbZf0cK3uKElrJD2Z90d2so/tIGm2pHWSHpX0iKQPZ33RY5d0sKRvS/pujvszWX+spHvz9X5jnuxQHElTJN0v6Ru5XPy4JW2Q9JCkByT1Zl3LXucTNuhrP7VwJnAicIGkEzvbq7a5BjhjSN1SYG1EzAHW5nJp9gAfjYgTgbnAxflvXPrYfwHMi4iTgJOBMyTNBS4DLo+I44CngcUd7GM7fRh4rLY8Wcb99og4uXbufMte5xM26Kn91EJE/BIY/KmF4kTEncCPh1QvAFZkeQWwcJ92ah+IiC0RcV+Wf0L1xz+Twsceld25eGDeApgH3JT1xY0bQNIs4GzgS7ksJsG4R9Cy1/lEDvrhfmphZof60gkzImJLlrcCMzrZmXaT1A28EbiXSTD2nL54ANgOrAG+B+yMiD3ZpNTX+xeAjwPP5fJ0Jse4A/gnSevz52Ggha9z/x59ASIiJBV7nqykw4GvAx+JiGeqg7xKqWOPiF8DJ0uaBtwCnNDhLrWdpHcB2yNivaTTO92ffeytEdEv6ZXAGkmP11eO93U+kY/oJ/tPLWyTdAxA3m/vcH/aQtKBVCF/XUTcnNWTYuwAEbETWAe8BZgmafDgrMTX+2nAuZI2UE3FzqO6rkXp4yYi+vN+O9Ub+6m08HU+kYN+sv/UwmpgUZYXAas62Je2yPnZq4HHIuLztVVFj11SVx7JI+kQqms7PEYV+O/OZsWNOyL+JCJmRUQ31d/z7RHxXgoft6TDJL18sAz8LvAwLXydT+hvxko6i2pOb/CnFj7b4S61haTrgdOpfrZ0G3Ap8L+AlcCrgY3AeREx9APbCU3SW4FvAQ/xwpztJ6nm6Ysdu6TfpvrwbQrVwdjKiPhzSa+lOtI9CrgfeF9E/KJzPW2fnLr5WES8q/Rx5/huycWpwD9GxGclTadFr/MJHfRmZja6iTx1Y2ZmDXDQm5kVzkFvZlY4B72ZWeEc9GZmhXPQm5kVzkFvZla4/w/awuSlohogaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb40c8e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We must also determine the maximum sentence size. To do this, we look at a histogram of text lenghts in the data set.\n",
    "#We see that a good cut-off might be around 25 words. \n",
    "\n",
    "#Plot histogram of text lengths \n",
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "sentence_size = 25\n",
    "min_word_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow has a built-in processing tool for determining vocabulary embedding, called VocabularyProcessor(), under the\n",
    "#learn.processing library\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will split the data into a train and test set\n",
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8),replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we declare the embedding matrix for the words. Sentence words will be translated into indices. These indices will be\n",
    "#translated into one-hot encoded vectors that we can create with an identity matrix, which will be the size of our word\n",
    "#embeddings. We will use this matrix to look up the sparse vector for each word and add them together for the sparse\n",
    "#sentence vector. \n",
    "identitiy_mat = tf.diag(tf.ones(shape=[embedding_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we will end up doing logistic regression to predict the probability of spam, we need to declare our logistic \n",
    "#regression variables. Then we declare our data placeholders as well. It is important to note that the x_data input \n",
    "#placeholder should be of integer type because it will be used to look up the row index of our identity matrix and \n",
    "#TensorFlow requires that lookup to be an integer\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "#Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1,1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we use TensorFlow's embedding lookup function that will map the indices of the words in the sentence to the one-hot\n",
    "#encoded vectors of our identity matrix. When we have that matrix, we create the sentence vector by summing up the \n",
    "#aforementioned word vectors. \n",
    "x_embed = tf.nn.embedding_lookup(identitiy_mat, x_data)\n",
    "x_cool_sums = tf.reduce_sum(x_embed,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have our fixed-length sentece vectors for each sentence, we want to perform logistic regression. To do this,\n",
    "#we will need to declare the actual model operations. Since we are doing this one data point at a time (stochastic\n",
    "#training), we will expand the dimensions of our input and perform linear regression operations on it. Remember that\n",
    "#TensorFlow has a loss function that includes the sigmoid function, so we do not need to include it in our output here\n",
    "x_col_sums_2D = tf.expand_dims(x_cool_sums,0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D,A),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now declare the loss function, prediction operation, and optimization function for training the model. \n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_target, logits=model_output))\n",
    "#Predictions operation\n",
    "prediction = tf.sigmoid(model_output)\n",
    "#Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we initialize our graph variables before we start the training generations:\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #10Loss = 1.4222957e-07\n",
      "Training Observation #20Loss = 8.321655e-17\n",
      "Training Observation #30Loss = 3.818709e-15\n",
      "Training Observation #40Loss = 0.0068280175\n",
      "Training Observation #50Loss = 8.7791365e-16\n",
      "Training Observation #60Loss = 1.2487995e-16\n",
      "Training Observation #70Loss = 4.320438e-21\n",
      "Training Observation #80Loss = 0.00157549\n",
      "Training Observation #90Loss = 7.6291503e-06\n",
      "Training Observation #100Loss = 9.835172e-15\n",
      "Training Observation #110Loss = 3.1028654e-19\n",
      "Training Observation #120Loss = 4.3921784e-05\n",
      "Training Observation #130Loss = 5.3654784e-14\n",
      "Training Observation #140Loss = 3.8060903e-20\n",
      "Training Observation #150Loss = 2.4353179e-08\n",
      "Training Observation #160Loss = 2.730738\n",
      "Training Observation #170Loss = 6.011515e-18\n",
      "Training Observation #180Loss = 1.06044495e-16\n",
      "Training Observation #190Loss = 3.631601e-21\n",
      "Training Observation #200Loss = 2.9936177e-20\n",
      "Training Observation #210Loss = 3.300642\n",
      "Training Observation #220Loss = 7.0960986e-15\n",
      "Training Observation #230Loss = 2.4535668e-19\n",
      "Training Observation #240Loss = 3.5339607e-14\n",
      "Training Observation #250Loss = 0.01922808\n",
      "Training Observation #260Loss = 14.177775\n",
      "Training Observation #270Loss = 2.1424977e-19\n",
      "Training Observation #280Loss = 1.4033502e-05\n",
      "Training Observation #290Loss = 1.6544374e-13\n",
      "Training Observation #300Loss = 9.633826e-06\n",
      "Training Observation #310Loss = 2.1908065e-18\n",
      "Training Observation #320Loss = 2.4832694e-12\n",
      "Training Observation #330Loss = 23.305712\n",
      "Training Observation #340Loss = 2.4330444e-18\n",
      "Training Observation #350Loss = 1.3513093e-17\n",
      "Training Observation #360Loss = 0.0008900724\n",
      "Training Observation #370Loss = 8.083401e-16\n",
      "Training Observation #380Loss = 3.032451e-15\n",
      "Training Observation #390Loss = 15.287468\n",
      "Training Observation #400Loss = 2.5291723e-20\n",
      "Training Observation #410Loss = 1.2040034e-18\n",
      "Training Observation #420Loss = 2.0997311e-16\n",
      "Training Observation #430Loss = 3.468013e-06\n",
      "Training Observation #440Loss = 2.6609378e-17\n",
      "Training Observation #450Loss = 7.0040335e-10\n",
      "Training Observation #460Loss = 1.0587673e-09\n",
      "Training Observation #470Loss = 1.7421524e-17\n",
      "Training Observation #480Loss = 1.088327e-06\n",
      "Training Observation #490Loss = 1.9733795e-17\n",
      "Training Observation #500Loss = 4.0302425e-07\n",
      "Training Observation #510Loss = 1.5176629e-11\n",
      "Training Observation #520Loss = 0.00063973514\n",
      "Training Observation #530Loss = 5.219449e-18\n",
      "Training Observation #540Loss = 2.9954026\n",
      "Training Observation #550Loss = 6.7157083\n",
      "Training Observation #560Loss = 4.724909e-16\n",
      "Training Observation #570Loss = 5.5732083\n",
      "Training Observation #580Loss = 1.5861187e-12\n",
      "Training Observation #590Loss = 6.26317e-11\n",
      "Training Observation #600Loss = 3.504067e-06\n",
      "Training Observation #610Loss = 21.727188\n",
      "Training Observation #620Loss = 4.606757e-09\n",
      "Training Observation #630Loss = 17.91229\n",
      "Training Observation #640Loss = 5.981704e-12\n",
      "Training Observation #650Loss = 1.1504575e-11\n",
      "Training Observation #660Loss = 1.3589746e-05\n",
      "Training Observation #670Loss = 5.985413\n",
      "Training Observation #680Loss = 13.915941\n",
      "Training Observation #690Loss = 5.3160424\n",
      "Training Observation #700Loss = 12.645192\n",
      "Training Observation #710Loss = 16.888407\n",
      "Training Observation #720Loss = 1.188568e-10\n",
      "Training Observation #730Loss = 5.943709e-15\n",
      "Training Observation #740Loss = 1.8718237e-11\n",
      "Training Observation #750Loss = 1.0075019e-05\n",
      "Training Observation #760Loss = 0.058211144\n",
      "Training Observation #770Loss = 0.0014249431\n",
      "Training Observation #780Loss = 8.96957e-12\n",
      "Training Observation #790Loss = 7.4929773e-10\n",
      "Training Observation #800Loss = 5.9117576e-07\n",
      "Training Observation #810Loss = 2.856859e-11\n",
      "Training Observation #820Loss = 0.01805106\n",
      "Training Observation #830Loss = 3.031922e-07\n",
      "Training Observation #840Loss = 15.92826\n",
      "Training Observation #850Loss = 2.2848683e-13\n",
      "Training Observation #860Loss = 0.00010193542\n",
      "Training Observation #870Loss = 3.6219175e-13\n",
      "Training Observation #880Loss = 2.6799732e-06\n",
      "Training Observation #890Loss = 3.931178e-10\n",
      "Training Observation #900Loss = 7.8111226e-13\n",
      "Training Observation #910Loss = 1.3561974e-07\n",
      "Training Observation #920Loss = 9.890248\n",
      "Training Observation #930Loss = 1.3425798e-09\n",
      "Training Observation #940Loss = 7.5289793\n",
      "Training Observation #950Loss = 3.234405e-08\n",
      "Training Observation #960Loss = 2.0420021e-10\n",
      "Training Observation #970Loss = 0.0794044\n",
      "Training Observation #980Loss = 1.0904328e-11\n",
      "Training Observation #990Loss = 5.5074226e-11\n",
      "Training Observation #1000Loss = 5.6873846e-06\n",
      "Training Observation #1010Loss = 0.94178045\n",
      "Training Observation #1020Loss = 9.700532\n",
      "Training Observation #1030Loss = 6.6965587e-12\n",
      "Training Observation #1040Loss = 3.0290838e-07\n",
      "Training Observation #1050Loss = 0.00016297764\n",
      "Training Observation #1060Loss = 3.155859\n",
      "Training Observation #1070Loss = 0.037910096\n",
      "Training Observation #1080Loss = 4.183515\n",
      "Training Observation #1090Loss = 6.29478e-13\n",
      "Training Observation #1100Loss = 3.251712e-12\n",
      "Training Observation #1110Loss = 1.6517085e-11\n",
      "Training Observation #1120Loss = 5.710118e-10\n",
      "Training Observation #1130Loss = 9.839559e-06\n",
      "Training Observation #1140Loss = 0.08317355\n",
      "Training Observation #1150Loss = 2.0635313e-11\n",
      "Training Observation #1160Loss = 9.597753e-05\n",
      "Training Observation #1170Loss = 1.278163e-12\n",
      "Training Observation #1180Loss = 9.327595\n",
      "Training Observation #1190Loss = 1.5607668e-07\n",
      "Training Observation #1200Loss = 9.678323e-10\n",
      "Training Observation #1210Loss = 1.7142814\n",
      "Training Observation #1220Loss = 9.36456e-08\n",
      "Training Observation #1230Loss = 6.021454e-10\n",
      "Training Observation #1240Loss = 6.680641e-11\n",
      "Training Observation #1250Loss = 0.4396073\n",
      "Training Observation #1260Loss = 2.3329216e-09\n",
      "Training Observation #1270Loss = 6.0145743e-07\n",
      "Training Observation #1280Loss = 4.9973883e-11\n",
      "Training Observation #1290Loss = 2.8270852e-10\n",
      "Training Observation #1300Loss = 0.051618222\n",
      "Training Observation #1310Loss = 0.0002704492\n",
      "Training Observation #1320Loss = 2.6068168\n",
      "Training Observation #1330Loss = 1.907868e-06\n",
      "Training Observation #1340Loss = 0.002448223\n",
      "Training Observation #1350Loss = 2.5751365e-10\n",
      "Training Observation #1360Loss = 7.8886425e-11\n",
      "Training Observation #1370Loss = 2.1586823e-09\n",
      "Training Observation #1380Loss = 1.3603212e-10\n",
      "Training Observation #1390Loss = 4.8424986e-11\n",
      "Training Observation #1400Loss = 0.00012148635\n",
      "Training Observation #1410Loss = 2.7542563e-07\n",
      "Training Observation #1420Loss = 9.519851e-05\n",
      "Training Observation #1430Loss = 3.5984674\n",
      "Training Observation #1440Loss = 0.020241402\n",
      "Training Observation #1450Loss = 8.825573e-09\n",
      "Training Observation #1460Loss = 0.9406052\n",
      "Training Observation #1470Loss = 1.2859828e-09\n",
      "Training Observation #1480Loss = 6.7445285e-11\n",
      "Training Observation #1490Loss = 8.092181e-08\n",
      "Training Observation #1500Loss = 2.4469582e-08\n",
      "Training Observation #1510Loss = 2.5861304e-05\n",
      "Training Observation #1520Loss = 6.261838e-09\n",
      "Training Observation #1530Loss = 2.3951128e-07\n",
      "Training Observation #1540Loss = 2.1912709e-10\n",
      "Training Observation #1550Loss = 0.004746113\n",
      "Training Observation #1560Loss = 6.151441e-08\n",
      "Training Observation #1570Loss = 3.8567423e-06\n",
      "Training Observation #1580Loss = 6.773846e-06\n",
      "Training Observation #1590Loss = 9.004751\n",
      "Training Observation #1600Loss = 0.00026043007\n",
      "Training Observation #1610Loss = 1.5873438e-09\n",
      "Training Observation #1620Loss = 3.908149e-07\n",
      "Training Observation #1630Loss = 2.1587935e-07\n",
      "Training Observation #1640Loss = 4.972665e-08\n",
      "Training Observation #1650Loss = 2.5938582e-06\n",
      "Training Observation #1660Loss = 2.364442e-08\n",
      "Training Observation #1670Loss = 12.812272\n",
      "Training Observation #1680Loss = 9.0698774e-07\n",
      "Training Observation #1690Loss = 2.8509102\n",
      "Training Observation #1700Loss = 1.696496e-05\n",
      "Training Observation #1710Loss = 6.3988895\n",
      "Training Observation #1720Loss = 1.2745655e-06\n",
      "Training Observation #1730Loss = 5.9127373e-08\n",
      "Training Observation #1740Loss = 4.978414\n",
      "Training Observation #1750Loss = 1.4793653e-07\n",
      "Training Observation #1760Loss = 0.00020203982\n",
      "Training Observation #1770Loss = 2.5518061e-06\n",
      "Training Observation #1780Loss = 4.918631e-09\n",
      "Training Observation #1790Loss = 6.243254e-05\n",
      "Training Observation #1800Loss = 1.7686446e-07\n",
      "Training Observation #1810Loss = 0.0030030673\n",
      "Training Observation #1820Loss = 4.9479637\n",
      "Training Observation #1830Loss = 1.2107056e-06\n",
      "Training Observation #1840Loss = 7.850739e-08\n",
      "Training Observation #1850Loss = 1.5822532e-06\n",
      "Training Observation #1860Loss = 4.601897e-06\n",
      "Training Observation #1870Loss = 8.2637007e-07\n",
      "Training Observation #1880Loss = 0.00011290943\n",
      "Training Observation #1890Loss = 8.5214815e-05\n",
      "Training Observation #1900Loss = 7.847146e-08\n",
      "Training Observation #1910Loss = 8.3737586e-07\n",
      "Training Observation #1920Loss = 3.5907125\n",
      "Training Observation #1930Loss = 0.008262755\n",
      "Training Observation #1940Loss = 2.4657092\n",
      "Training Observation #1950Loss = 1.523911e-07\n",
      "Training Observation #1960Loss = 4.76347\n",
      "Training Observation #1970Loss = 2.573572e-06\n",
      "Training Observation #1980Loss = 0.0028606965\n",
      "Training Observation #1990Loss = 2.4682267\n",
      "Training Observation #2000Loss = 0.0013141235\n",
      "Training Observation #2010Loss = 0.00020115344\n",
      "Training Observation #2020Loss = 1.8487553e-07\n",
      "Training Observation #2030Loss = 6.0596703e-06\n",
      "Training Observation #2040Loss = 0.13621488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #2050Loss = 9.314356\n",
      "Training Observation #2060Loss = 4.5306003e-08\n",
      "Training Observation #2070Loss = 0.00018066195\n",
      "Training Observation #2080Loss = 3.9973664e-05\n",
      "Training Observation #2090Loss = 0.00024359321\n",
      "Training Observation #2100Loss = 5.8520723e-06\n",
      "Training Observation #2110Loss = 4.5827317e-05\n",
      "Training Observation #2120Loss = 3.1245367e-05\n",
      "Training Observation #2130Loss = 0.0017744129\n",
      "Training Observation #2140Loss = 9.4486177e-07\n",
      "Training Observation #2150Loss = 1.6634052e-05\n",
      "Training Observation #2160Loss = 0.037913173\n",
      "Training Observation #2170Loss = 2.6710384\n",
      "Training Observation #2180Loss = 1.0150502\n",
      "Training Observation #2190Loss = 1.714957e-05\n",
      "Training Observation #2200Loss = 0.00013392122\n",
      "Training Observation #2210Loss = 8.559434e-06\n",
      "Training Observation #2220Loss = 3.9264945e-05\n",
      "Training Observation #2230Loss = 1.3204034e-05\n",
      "Training Observation #2240Loss = 0.19104438\n",
      "Training Observation #2250Loss = 0.0152364625\n",
      "Training Observation #2260Loss = 0.008566551\n",
      "Training Observation #2270Loss = 1.9396487e-06\n",
      "Training Observation #2280Loss = 0.00031722206\n",
      "Training Observation #2290Loss = 0.00053603906\n",
      "Training Observation #2300Loss = 0.00040827296\n",
      "Training Observation #2310Loss = 6.2653675\n",
      "Training Observation #2320Loss = 0.00023658136\n",
      "Training Observation #2330Loss = 0.00090181804\n",
      "Training Observation #2340Loss = 0.0009081552\n",
      "Training Observation #2350Loss = 0.00029960726\n",
      "Training Observation #2360Loss = 0.33026293\n",
      "Training Observation #2370Loss = 6.952843e-05\n",
      "Training Observation #2380Loss = 1.2298063e-05\n",
      "Training Observation #2390Loss = 0.004416421\n",
      "Training Observation #2400Loss = 0.000474578\n",
      "Training Observation #2410Loss = 4.9769506\n",
      "Training Observation #2420Loss = 0.0039059464\n",
      "Training Observation #2430Loss = 3.3102486e-05\n",
      "Training Observation #2440Loss = 8.0363525e-06\n",
      "Training Observation #2450Loss = 3.9881288e-05\n",
      "Training Observation #2460Loss = 1.3212058e-05\n",
      "Training Observation #2470Loss = 0.0010633924\n",
      "Training Observation #2480Loss = 2.62162e-05\n",
      "Training Observation #2490Loss = 0.0002661515\n",
      "Training Observation #2500Loss = 0.0003266432\n",
      "Training Observation #2510Loss = 0.012477162\n",
      "Training Observation #2520Loss = 11.281592\n",
      "Training Observation #2530Loss = 0.07413474\n",
      "Training Observation #2540Loss = 0.011358416\n",
      "Training Observation #2550Loss = 8.9065186e-05\n",
      "Training Observation #2560Loss = 0.0032142887\n",
      "Training Observation #2570Loss = 0.00015877366\n",
      "Training Observation #2580Loss = 0.00010113467\n",
      "Training Observation #2590Loss = 2.6860209\n",
      "Training Observation #2600Loss = 0.008044782\n",
      "Training Observation #2610Loss = 1.7322376e-05\n",
      "Training Observation #2620Loss = 0.008823023\n",
      "Training Observation #2630Loss = 0.0037154413\n",
      "Training Observation #2640Loss = 0.0015895317\n",
      "Training Observation #2650Loss = 0.028172815\n",
      "Training Observation #2660Loss = 5.5752275e-06\n",
      "Training Observation #2670Loss = 5.3107167e-05\n",
      "Training Observation #2680Loss = 1.7551893\n",
      "Training Observation #2690Loss = 8.360638e-05\n",
      "Training Observation #2700Loss = 0.00062325166\n",
      "Training Observation #2710Loss = 0.040356398\n",
      "Training Observation #2720Loss = 1.9427353e-05\n",
      "Training Observation #2730Loss = 0.78843236\n",
      "Training Observation #2740Loss = 1.6979948e-05\n",
      "Training Observation #2750Loss = 5.296444e-05\n",
      "Training Observation #2760Loss = 0.0052045034\n",
      "Training Observation #2770Loss = 4.8747344\n",
      "Training Observation #2780Loss = 5.7171016\n",
      "Training Observation #2790Loss = 0.002736692\n",
      "Training Observation #2800Loss = 1.1813407e-05\n",
      "Training Observation #2810Loss = 4.1446943\n",
      "Training Observation #2820Loss = 2.15355e-05\n",
      "Training Observation #2830Loss = 0.0006786502\n",
      "Training Observation #2840Loss = 1.1294355\n",
      "Training Observation #2850Loss = 0.017852848\n",
      "Training Observation #2860Loss = 0.00022921634\n",
      "Training Observation #2870Loss = 0.21481968\n",
      "Training Observation #2880Loss = 0.006588606\n",
      "Training Observation #2890Loss = 0.00023762636\n",
      "Training Observation #2900Loss = 0.44823724\n",
      "Training Observation #2910Loss = 4.68252e-05\n",
      "Training Observation #2920Loss = 0.0001605607\n",
      "Training Observation #2930Loss = 0.4040776\n",
      "Training Observation #2940Loss = 0.00018927334\n",
      "Training Observation #2950Loss = 0.007792131\n",
      "Training Observation #2960Loss = 0.1312072\n",
      "Training Observation #2970Loss = 3.7189668e-05\n",
      "Training Observation #2980Loss = 0.00013660551\n",
      "Training Observation #2990Loss = 0.7009687\n",
      "Training Observation #3000Loss = 7.64769e-06\n",
      "Training Observation #3010Loss = 0.0001495838\n",
      "Training Observation #3020Loss = 2.034758\n",
      "Training Observation #3030Loss = 0.1813127\n",
      "Training Observation #3040Loss = 0.028897703\n",
      "Training Observation #3050Loss = 0.001706847\n",
      "Training Observation #3060Loss = 0.00020233038\n",
      "Training Observation #3070Loss = 0.00034870693\n",
      "Training Observation #3080Loss = 0.003877712\n",
      "Training Observation #3090Loss = 0.47195828\n",
      "Training Observation #3100Loss = 10.076076\n",
      "Training Observation #3110Loss = 0.08218454\n",
      "Training Observation #3120Loss = 5.792422\n",
      "Training Observation #3130Loss = 1.975888e-05\n",
      "Training Observation #3140Loss = 0.54074246\n",
      "Training Observation #3150Loss = 0.0052214866\n",
      "Training Observation #3160Loss = 0.005211986\n",
      "Training Observation #3170Loss = 0.0054295557\n",
      "Training Observation #3180Loss = 0.0024745136\n",
      "Training Observation #3190Loss = 0.564248\n",
      "Training Observation #3200Loss = 2.686319\n",
      "Training Observation #3210Loss = 0.005303442\n",
      "Training Observation #3220Loss = 0.1595295\n",
      "Training Observation #3230Loss = 0.0014696501\n",
      "Training Observation #3240Loss = 0.0011314322\n",
      "Training Observation #3250Loss = 8.087699e-05\n",
      "Training Observation #3260Loss = 1.4950893\n",
      "Training Observation #3270Loss = 0.008996902\n",
      "Training Observation #3280Loss = 0.00012033702\n",
      "Training Observation #3290Loss = 10.291935\n",
      "Training Observation #3300Loss = 0.0012396296\n",
      "Training Observation #3310Loss = 0.63880956\n",
      "Training Observation #3320Loss = 0.0050969343\n",
      "Training Observation #3330Loss = 0.0009555666\n",
      "Training Observation #3340Loss = 0.0014473425\n",
      "Training Observation #3350Loss = 0.004430961\n",
      "Training Observation #3360Loss = 0.90334135\n",
      "Training Observation #3370Loss = 1.4072562\n",
      "Training Observation #3380Loss = 0.006847224\n",
      "Training Observation #3390Loss = 0.06621171\n",
      "Training Observation #3400Loss = 3.4405006e-05\n",
      "Training Observation #3410Loss = 0.0012724383\n",
      "Training Observation #3420Loss = 8.067858\n",
      "Training Observation #3430Loss = 0.034124\n",
      "Training Observation #3440Loss = 0.0010535611\n",
      "Training Observation #3450Loss = 12.565931\n",
      "Training Observation #3460Loss = 0.926249\n",
      "Training Observation #3470Loss = 0.015227056\n",
      "Training Observation #3480Loss = 3.5194593e-05\n",
      "Training Observation #3490Loss = 0.0001628813\n",
      "Training Observation #3500Loss = 1.1566643\n",
      "Training Observation #3510Loss = 4.3280516\n",
      "Training Observation #3520Loss = 0.0023686842\n",
      "Training Observation #3530Loss = 0.00035125125\n",
      "Training Observation #3540Loss = 0.006913284\n",
      "Training Observation #3550Loss = 0.00017968609\n",
      "Training Observation #3560Loss = 0.00018543784\n",
      "Training Observation #3570Loss = 0.0014648814\n",
      "Training Observation #3580Loss = 12.783476\n",
      "Training Observation #3590Loss = 0.5668674\n",
      "Training Observation #3600Loss = 0.8068522\n",
      "Training Observation #3610Loss = 7.481816\n",
      "Training Observation #3620Loss = 0.34588015\n",
      "Training Observation #3630Loss = 0.43443495\n",
      "Training Observation #3640Loss = 0.125462\n",
      "Training Observation #3650Loss = 0.0006791835\n",
      "Training Observation #3660Loss = 0.98320276\n",
      "Training Observation #3670Loss = 0.012992334\n",
      "Training Observation #3680Loss = 0.67145425\n",
      "Training Observation #3690Loss = 0.002165383\n",
      "Training Observation #3700Loss = 0.0021197493\n",
      "Training Observation #3710Loss = 0.10940046\n",
      "Training Observation #3720Loss = 4.153553\n",
      "Training Observation #3730Loss = 0.0011958516\n",
      "Training Observation #3740Loss = 0.00017050144\n",
      "Training Observation #3750Loss = 1.6218925\n",
      "Training Observation #3760Loss = 0.052905027\n",
      "Training Observation #3770Loss = 0.0048877806\n",
      "Training Observation #3780Loss = 0.00080912694\n",
      "Training Observation #3790Loss = 0.17396493\n",
      "Training Observation #3800Loss = 0.00011364344\n",
      "Training Observation #3810Loss = 0.0011113068\n",
      "Training Observation #3820Loss = 3.2436473\n",
      "Training Observation #3830Loss = 5.3626237\n",
      "Training Observation #3840Loss = 0.00012411739\n",
      "Training Observation #3850Loss = 0.0014961674\n",
      "Training Observation #3860Loss = 0.051187046\n",
      "Training Observation #3870Loss = 11.227786\n",
      "Training Observation #3880Loss = 3.4737164e-07\n",
      "Training Observation #3890Loss = 0.27704865\n",
      "Training Observation #3900Loss = 4.173031\n",
      "Training Observation #3910Loss = 0.0012418322\n",
      "Training Observation #3920Loss = 0.004807917\n",
      "Training Observation #3930Loss = 0.02532795\n",
      "Training Observation #3940Loss = 0.00048108757\n",
      "Training Observation #3950Loss = 0.13112257\n",
      "Training Observation #3960Loss = 1.1754117\n",
      "Training Observation #3970Loss = 0.019382272\n",
      "Training Observation #3980Loss = 0.010604961\n",
      "Training Observation #3990Loss = 4.1007967\n",
      "Training Observation #4000Loss = 0.022245813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #4010Loss = 0.016467514\n",
      "Training Observation #4020Loss = 0.031168135\n",
      "Training Observation #4030Loss = 0.0053419736\n",
      "Training Observation #4040Loss = 0.0014103227\n",
      "Training Observation #4050Loss = 0.00015734584\n",
      "Training Observation #4060Loss = 2.2111783\n",
      "Training Observation #4070Loss = 0.025554061\n",
      "Training Observation #4080Loss = 0.0015287985\n",
      "Training Observation #4090Loss = 5.8228326\n",
      "Training Observation #4100Loss = 0.000100050784\n",
      "Training Observation #4110Loss = 1.3144858e-05\n",
      "Training Observation #4120Loss = 0.0011448958\n",
      "Training Observation #4130Loss = 0.017944109\n",
      "Training Observation #4140Loss = 0.00048120637\n",
      "Training Observation #4150Loss = 7.1171424e-05\n",
      "Training Observation #4160Loss = 1.827994\n",
      "Training Observation #4170Loss = 0.00014997729\n",
      "Training Observation #4180Loss = 0.044045504\n",
      "Training Observation #4190Loss = 0.00039817183\n",
      "Training Observation #4200Loss = 0.0012390119\n",
      "Training Observation #4210Loss = 0.00092549523\n",
      "Training Observation #4220Loss = 0.026032109\n",
      "Training Observation #4230Loss = 0.00015709442\n",
      "Training Observation #4240Loss = 9.698275\n",
      "Training Observation #4250Loss = 0.009403689\n",
      "Training Observation #4260Loss = 0.012814288\n",
      "Training Observation #4270Loss = 9.041369e-07\n",
      "Training Observation #4280Loss = 7.4714856\n",
      "Training Observation #4290Loss = 9.559493\n",
      "Training Observation #4300Loss = 0.3296965\n",
      "Training Observation #4310Loss = 0.0009754316\n",
      "Training Observation #4320Loss = 0.008607484\n",
      "Training Observation #4330Loss = 1.7156507\n",
      "Training Observation #4340Loss = 0.006910032\n",
      "Training Observation #4350Loss = 0.00055952265\n",
      "Training Observation #4360Loss = 0.003978989\n",
      "Training Observation #4370Loss = 0.0013169432\n",
      "Training Observation #4380Loss = 0.021552242\n",
      "Training Observation #4390Loss = 2.6109512e-05\n",
      "Training Observation #4400Loss = 0.0013294275\n",
      "Training Observation #4410Loss = 0.0007141111\n",
      "Training Observation #4420Loss = 0.0027698798\n",
      "Training Observation #4430Loss = 0.002344212\n",
      "Training Observation #4440Loss = 0.009653782\n",
      "Training Observation #4450Loss = 0.072990015\n"
     ]
    }
   ],
   "source": [
    "#Now we start the iteration over the sentences. TensorFlow's vocab_processor.fit() function is a generator that operates\n",
    "#one sentence at a time. We will use this to our advantage to do stochastic training on our logistic model. To get a \n",
    "#better idea of the accuracy trend, we keep a trailing average of the past 50 training steps. If we just plotted the \n",
    "#current one, we would either see 1 or 0 depending on whether we predicted that training data point currently or not.\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    \n",
    "    sess.run(train_step, feed_dict={x_data:t,y_target:y_data})\n",
    "    \n",
    "    temp_loss = sess.run(loss, feed_dict={x_data:t, y_target:y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    if(ix+1)%10==0:\n",
    "        print('Training Observation #' + str(ix+1) + 'Loss = ' + str(temp_loss))\n",
    "        \n",
    "        #Keep trailing average of past 50 observations accuracy\n",
    "        #Get prediction of single observation \n",
    "        [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "        #Get True/False if prediction is accurate\n",
    "        train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "        train_acc_all.append(train_acc_temp)\n",
    "        if len(train_acc_all) >=50:\n",
    "            train_acc_avg.append(np.mean(train_acc_all[-50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Test Set Accuracy\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "\n",
      "Overall Test Accuracy: 0.8278026905829596\n"
     ]
    }
   ],
   "source": [
    "#To get the test set accuracy, we repeat the preceding process, but only on the prediction operation, not the training \n",
    "#operation with the test texts:\n",
    "print('Getting Test Set Accuracy')\n",
    "test_acc_all = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    \n",
    "    if(ix+1)%50==0:\n",
    "        print('Test Observation #' + str(ix+1))\n",
    "        \n",
    "    #Keep trailing average of past 50 observations accuracy\n",
    "    #Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t,y_target:y_data})\n",
    "    #Get True/False if prediction is accurate\n",
    "    test_acc_temp = target_test[ix]==np.round(temp_pred)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "print('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import os \n",
    "import pickle \n",
    "import string\n",
    "import requests\n",
    "import collections \n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "sess = tf.Session()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_name = 'temp'\n",
    "if not os.path.exists(data_folder_name):\n",
    "    os.makedirs(data_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare model parameters \n",
    "batch_size = 200\n",
    "embedding_size = 50\n",
    "vocabulary_size = 2000\n",
    "generations = 50000\n",
    "model_learning_rate = 0.05\n",
    "num_sampled = int(batch_size/2)\n",
    "window_size = 3\n",
    "#Add checkpoints t training \n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 1000\n",
    "#Declare stop words\n",
    "stops = stopwords.words('english')\n",
    "#We pick some test words. We are expecting synonyms to appear\n",
    "valid_words = ['love', 'hate', 'happy', 'sad', 'man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we declare our data loading function, which checks to make sure we have not downloaded the data before it downloads,\n",
    "#or it will load the data from the disk if we have saved it before\n",
    "def load_movie_data():\n",
    "    save_folder_name = 'rt-polaritydata'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
    "\n",
    "    # Check if files are already downloaded\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "\n",
    "        # Save tar.gz file\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        # Extract tar.gz file into temp folder\n",
    "        tar = tarfile.open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path='temp')\n",
    "        tar.close()\n",
    "\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    pos_data = [x.rstrip() for x in pos_data]\n",
    "\n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    neg_data = [x.rstrip() for x in neg_data]\n",
    "    \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    \n",
    "    return(texts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we crate a normalization function for text. This function will input a list of strings and apply lowecasing, remove\n",
    "#punctiation, remove numbers, trim extra whitespace, and remove stop words. \n",
    "def normalize_text(texts, stops):\n",
    "    #Lower case\n",
    "    texts = [x.lower() for x in texts]\n",
    "    #Remove punctuation\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    #Remove numbers\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    #Remove stopwords\n",
    "    texts = [' '.join(word for word in x.split() if word not in (stops)) for x in texts]\n",
    "    #Trim extra whitespace\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Normalizing Text Data\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Declare stop words\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# We pick some test words. We are expecting synonyms to appear\n",
    "valid_words = ['love', 'hate', 'happy', 'sad', 'man', 'woman']\n",
    "# Later we will have to transform these into indices\n",
    "\n",
    "# Load the movie review data\n",
    "print('Loading Data')\n",
    "texts, target = load_movie_data()\n",
    "\n",
    "# Normalize text\n",
    "print('Normalizing Text Data')\n",
    "texts = normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To build our vocabulary, we create a function that creates a dictionary of words with their count, and any word that is\n",
    "#uncommon enough to not make our vocabulary size cut-off, we label as 'RARE'\n",
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    #Turn sentences (list of strings) into lists of words\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    #Initialize list of [word, word_count] for each word, starting with unknown\n",
    "    count = [['RARE', -1]]\n",
    "    #Now add most frequent words, limited to the N-most frequent(N=vocabulary size)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    #Now create the dictionary\n",
    "    word_dict = {}\n",
    "    #For each word, that we want in the dictionary, add it, then make it the value of the prior dictionary length\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need a function that will convert a list of sentences into lists of word indices that we can pass into our embedding \n",
    "#lookup function\n",
    "def text_to_numbers(sentences, word_dict):\n",
    "    #Initialize the returned data \n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        #For each word, either use selected index or rare word index\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dictionary\n"
     ]
    }
   ],
   "source": [
    "# Build our data set and dictionaries\n",
    "print('Creating Dictionary')\n",
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now create a function that will return our skip-gram batches. We want to train on pairs of words where one word\n",
    "#is the training input (from the target word at the center of our window) and the other word is selected from the \n",
    "#window. For example, the sentence \"the cat in the hat\" may result in (input,output) pairs such as the following:\n",
    "#(the,in), (cat,in), (the,in), (hat,in), if in was the target word, and we had a window size of two in each direction\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "        rand_sentence_ix = int(np.random.choice(len(sentences), size=1))\n",
    "        rand_sentence = sentences[rand_sentence_ix]\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "            batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Only keep windows with consistent 2*window_size\n",
    "            batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n",
    "            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
    "        elif method=='doc2vec':\n",
    "            # For doc2vec we keep LHS window only to predict target word\n",
    "            batch_and_labels = [(rand_sentence[i:i+window_size], rand_sentence[i+window_size]) for i in range(0, len(rand_sentence)-window_size)]\n",
    "            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
    "            # Add document index to batch!! Remember that we must extract the last index in batch for the doc-index\n",
    "            batch = [x + [rand_sentence_ix] for x in batch]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return(batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size, 2*window_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding\n",
    "# Add together window embeddings:\n",
    "embed = tf.zeros([batch_size, embedding_size])\n",
    "for element in range(2*window_size):\n",
    "    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the NCE loss function that TensorFlow has built in because categorical output is too sparse for the softmax to\n",
    "#converge, as follows\n",
    "\n",
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save our embeddings, we must load the TensorFlow train.Saver method. This method defaults to saving the whole graph,\n",
    "#but we can give it an argument just to save the embedding variable, and we can also give it a specific name. Here we give\n",
    "#it the same name as the variable name in our graph\n",
    "saver = tf.train.Saver({\"embeddings\":embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now declare an optimizer function and initialize our model variables.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out sentences that aren't long enough:\n",
    "text_data = [x for x in text_data if len(x)>=(2*window_size+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Loss at step 1000 : 3.1904115676879883\n",
      "Loss at step 2000 : 2.7858283519744873\n",
      "Loss at step 3000 : 2.5651252269744873\n",
      "Loss at step 4000 : 2.2575652599334717\n",
      "Loss at step 5000 : 2.155435562133789\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 6000 : 4.902319431304932\n",
      "Loss at step 7000 : 1.8063534498214722\n",
      "Loss at step 8000 : 1.8216465711593628\n",
      "Loss at step 9000 : 1.8166290521621704\n",
      "Loss at step 10000 : 1.774012804031372\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 11000 : 1.5938515663146973\n",
      "Loss at step 12000 : 1.6208261251449585\n",
      "Loss at step 13000 : 1.7160840034484863\n",
      "Loss at step 14000 : 1.4641553163528442\n",
      "Loss at step 15000 : 1.5309360027313232\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 16000 : 1.6634857654571533\n",
      "Loss at step 17000 : 1.5036532878875732\n",
      "Loss at step 18000 : 1.5509381294250488\n",
      "Loss at step 19000 : 1.4619667530059814\n",
      "Loss at step 20000 : 1.4321143627166748\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 21000 : 1.354300856590271\n",
      "Loss at step 22000 : 1.5816099643707275\n",
      "Loss at step 23000 : 1.3971575498580933\n",
      "Loss at step 24000 : 2.3993284702301025\n",
      "Loss at step 25000 : 1.4630486965179443\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 26000 : 1.4933311939239502\n",
      "Loss at step 27000 : 1.376326322555542\n",
      "Loss at step 28000 : 5.353203773498535\n",
      "Loss at step 29000 : 1.4172439575195312\n",
      "Loss at step 30000 : 1.303670048713684\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 31000 : 1.431996464729309\n",
      "Loss at step 32000 : 1.3716644048690796\n",
      "Loss at step 33000 : 3.3630619049072266\n",
      "Loss at step 34000 : 1.3516602516174316\n",
      "Loss at step 35000 : 1.3492114543914795\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 36000 : 1.387283205986023\n",
      "Loss at step 37000 : 1.342777132987976\n",
      "Loss at step 38000 : 1.3978090286254883\n",
      "Loss at step 39000 : 1.3120800256729126\n",
      "Loss at step 40000 : 1.492969036102295\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 41000 : 1.3402774333953857\n",
      "Loss at step 42000 : 1.2883193492889404\n",
      "Loss at step 43000 : 1.408907175064087\n",
      "Loss at step 44000 : 1.3655434846878052\n",
      "Loss at step 45000 : 1.3433994054794312\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n",
      "Loss at step 46000 : 1.370387077331543\n",
      "Loss at step 47000 : 1.4072375297546387\n",
      "Loss at step 48000 : 1.322435736656189\n",
      "Loss at step 49000 : 1.351130723953247\n",
      "Loss at step 50000 : 1.3782774209976196\n",
      "Nearest to love: ludicrous, lee, pleasant, format, smug,\n",
      "Nearest to hate: lazy, storytelling, cultural, wouldnt, big,\n",
      "Nearest to happy: anyway, toward, balance, robert, visually,\n",
      "Nearest to sad: memories, hits, got, simply, science,\n",
      "Nearest to man: way, played, making, guys, spectacular,\n",
      "Nearest to woman: importance, cinema, poor, harmless, material,\n",
      "Model saved in file: /home/huzeyfekiran/Downloads/Cookbook exercises/temp/cbow_movie_embeddings.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Finally, we can loop across our training step and print out the loss, and save the embeddings and dictionary when we \n",
    "#specify:\n",
    "#Now we can print our embeddings and print off the loss and closest words to our validations set during the training.\n",
    "print('Starting Training')\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size,\n",
    "                                                                  window_size, method='cbow')\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},' .format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    # Save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        # Save vocabulary dictionary\n",
    "        with open(os.path.join(data_folder_name,'movie_vocab.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        # Save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),data_folder_name,'cbow_movie_embeddings.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f398dec84a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(loss_x_vec, loss_vec, 'k-')\n",
    "plt.title('Training Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

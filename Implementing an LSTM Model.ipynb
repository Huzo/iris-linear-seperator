{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we start a graph session and set the RNN parameters\n",
    "sess = tf.Session()\n",
    "#Set RNN parameters\n",
    "min_word_freq = 5 # Trim the less frequent words off\n",
    "rnn_size = 128 # RNN Model size\n",
    "embedding_size = 100 # Word embedding size\n",
    "epochs = 10 # Number of epochs to cycle through data\n",
    "batch_size = 100 # Train on this many examples at once\n",
    "learning_rate = 0.001 # Learning rate\n",
    "training_seq_len = 50 # how long of a word group to consider \n",
    "embedding_size = rnn_size\n",
    "save_every = 500 # How often to save model checkpoints\n",
    "eval_every = 50 # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set up the data and model folders and filenames, along with declaring punctuation to remove. We will want to keep \n",
    "#hyphens and apostrophes because Shakespeare uses them frequently to combine words and syllables\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n",
      "Not found, downloading Shakespeare texts from www.gutenberg.org\n"
     ]
    }
   ],
   "source": [
    "#Next we get the data. If the data file does not exist, download and save the Shakespeare text. If it does exist, load it\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "print('Loading Shakespeare Data')\n",
    "# Check if file is downloaded.\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Text\n",
      "Done loading/cleaning.\n"
     ]
    }
   ],
   "source": [
    "#we clean the Shakespeare text by removing punctuation and extra whitespace. \n",
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()\n",
    "print('Done loading/cleaning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now deal with creating the Shakespeare vocabulary to use. We create a function that will return the two dictionaries\n",
    "#(word to index, and index to word) with words that appear more than a specified frequency\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    #limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    #Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    #Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unkown'] = 0\n",
    "    #Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key, val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have our vocabulary, we turn the Shakespeare text into an array of indices. \n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this recipe, we will show how to create a model in a class object. This will be helpful for us, because we would like\n",
    "#to use the same model (with the same weights) to train on batches and to generate text from sample text. This will\n",
    "#prove hard to do without a class with an internal sampling method. Ideally, this class code should sit in a seperate\n",
    "#Python file, which we can import at the beginning of this script\n",
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will declare the LSTM model as well as the test model. We will do this within a variable scope and tell the \n",
    "#scope that we will reuse the variables for the test LSTM model. \n",
    "\n",
    "# Define LSTM Model\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "\n",
    "# Tell TensorFlow we are reusing the scope for the testing\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a saving operation, as well as splitting up the input text into equal batch-size chunks. Then we will \n",
    "#initialize the variables of the model\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 10.01\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 9.24\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.75\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.43\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 8.15\n",
      "thou art more wrongfully cords doubts colours empire\n",
      "to be or not to\n",
      "wherefore art thou me- damsel folly grief feeling\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.94\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.76\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.59\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.18\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.19\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 7.14\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.91\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.76\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.64\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.72\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.71\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.35\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.58\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.56\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.49\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.49\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.40\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.30\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.17\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.48\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.43\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.23\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.37\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.40\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.28\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.12\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.33\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.18\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.31\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.22\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.22\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.60\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.44\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.36\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.53\n",
      "thou art more than than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.36\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.31\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.25\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.05\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.04\n",
      "thou art more than than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.18\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.20\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 5.97\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.25\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.05\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.21\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.36\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.31\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.05\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.39\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 5.99\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 6.13\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.19\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.11\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.00\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.10\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.00\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.23\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 5.88\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.34\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 5.97\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.16\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 5.89\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.20\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 6.01\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art thou art thou hast thou hast thou\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.13\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.05\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 5.98\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 5.80\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.10\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 6.03\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 6.11\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 6.11\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 5.94\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 6.18\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 5.90\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 6.01\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 5.78\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 6.07\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 5.98\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou hast thou shalt have\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 5.93\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 5.99\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 6.09\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 5.95\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 5.66\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art a\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 6.03\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 5.78\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.86\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.74\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 5.94\n",
      "thou art more than a\n",
      "to be or not to\n",
      "wherefore art thou art not a\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 5.99\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.63\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.93\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 6.05\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.90\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 6.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 5.87\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 6.04\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 6.09\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 6.01\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 5.97\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.74\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 5.79\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.70\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.89\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.66\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 6.01\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.78\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.92\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.82\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 6.18\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 6.10\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.82\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 6.12\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 6.03\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 5.52\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 5.83\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.92\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.92\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 5.91\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou know'st thou know'st thou hast thou\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 6.02\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.85\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 6.03\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.89\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.95\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou know'st thou art thou art not a\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.79\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.73\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 5.78\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 5.98\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.88\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou know'st thou hast thou wilt thou hast thou\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.77\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.80\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.82\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.61\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.98\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou know'st thou art a\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.59\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.97\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.85\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.97\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.74\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art a\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.77\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.75\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.73\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.90\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.94\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.97\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.75\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.72\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.78\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.88\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 5.72\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 5.70\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.94\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.79\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.93\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.96\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.72\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 6.07\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.53\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.71\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou know'st thou know'st thou hast thou\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.79\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.82\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.81\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.85\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.66\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou know'st thou know'st thou hast thou\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.93\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.79\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 5.72\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.85\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 5.84\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou know'st thou art thou art a\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.85\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.75\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 5.56\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.62\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.93\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.70\n"
     ]
    }
   ],
   "source": [
    "#We can now iterate through our epochs, shuffling the data before each epoch starts. The target for our data is just the\n",
    "#same data, but shifted by one value (using thenumpy.roll() funtion)\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89ba5802e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
